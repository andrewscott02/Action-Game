\documentclass{IEEEtran}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{url}
\usepackage{amsmath}
\usepackage{wrapfig, framed, caption}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=blue,      
    urlcolor=blue,
    citecolor=blue,
    pdfpagemode=FullScreen,
    }
\usepackage{tabularx}
\usepackage{tabularray}
\usepackage[hmargin=1cm]{geometry}
\usepackage{multirow}

%Dissertation Checklist

%Finish Research Proposal (Few sections missing, sort out citations)
%Ethics Proposal (Done, need to sort out handbook citation)

%Questionnaire (Done)
%Data Analysis Method (Started)
%R-Code (Started)

%Quality Assurance Test Plan (Not Started)

%Presentation (Put hypothesis and more info on how the player model is created)

%Prototype (on defensive actions, check if nearby characters are attacking)

%Your document must not exceed six pages of text, excluding figures, tables, references and appendices.
%This is subject to the usual policy on word and page limits available on LearningSpace.


%Useful links
%G-Power downloads
    %https://www.psychologie.hhu.de/arbeitsgruppen/allgemeine-psychologie-und-arbeitspsychologie/gpower
    %https://learningspace.falmouth.ac.uk/mod/resource/view.php?id=245797

%\title{Evaluating the use of Adaptive AI to Build Player-Companion Collaboration} 
%Evaluating Collaboration With AI NPCs to Build Player-Companion Relationships
\title{Does the Use of Adaptive AI Help to Build Player-Companion Collaboration?}
\author{Andrew J. Scott}  
\date{September 2022}

\begin{document}
	\maketitle
	\pagenumbering{arabic}

\begin{abstract}
This proposal demonstrates the use of adaptive AI for companion characters that can work with players in an action game. The companion cannot be explicitly commanded, so the AI will respond to player actions to collaborate with them better, which preserves independence. This adaption will be tested to determine if it the companion character gives the player a greater sense of teamwork.
\end{abstract}

 \begin{IEEEkeywords}
Artificial Intelligence, AI, Synergy, Collaboration, Cooperation, Companion, NPC, Adaptive, Game.
\end{IEEEkeywords}

\section{Introduction}
\label{Intro}

This proposal presents an AI companion that will aid the player in combat, similar to \textit{Atreus} in \textit{God of War} or \textit{Ellie} from \textit{The Last of Us}. The focus of this research would be on using agent modelling, outlined in section \ref{Communication}, to make an adaptive AI that determines how the companion can best assist the player without requiring any explicit commands from them.

In the next section, this proposal summarizes the motivations for the project. In section \ref{RelatedWork}, this proposal analyses common design practices for AI. Sections \ref{Responsive Behaviour} \ref{Movement} \ref{ABC} focus on various methods for AI in industry, while section \ref{Communication} focuses on analysing methods in academic research that are used to create adaptive AI. Section \ref{ProposedResearch} details the design of the experiment used in this research.

\section{Background}
\label{Background}

A key motivation for this work is a developer conference by Constantine et al. that focused on building the relationship between the players and their ally, Aurene, so that their death feels more meaningful \cite{EGXCharacterDeathGuildWars}. One of their points was that allowing players to explicitly command companions can ruin their agency.

When developing the AI for \textit{Atreus} in \textit{God of War}, there was a lot more of a focus on the sense of teamwork between Atreus and the player. This can be overshadowed as the player can explicitly command Atreus, which can ruin his agency \cite{EGXCharacterDeathGuildWars}. The intention for this research is to create an AI that can collaborate with the player like Atreus, but is independent like Aurene.

This research will build upon combat AI for companion characters by focusing on the sense of collaboration between them and the player while maintaining this independence. Collaboration in this research is defined as the sense of teamwork between the player and a companion NPC. This is built with synergistic behaviours where the AI makes it easier for the player to play the game in a noticeable way. To do this, this research presents an adaptive AI, which is where the companion will change their own behaviours in response to the player's actions. The intention is that responding to the player's actions will make them more collaborative.

The AI will have to balance multiple duties, such as helping the player stay safe, attacking other enemies so they do not get overwhelmed and maintaining their own safety \cite{CoupledEmpowermentMaximisation, tremblay2013adaptive}. To preserve player agency, the AI should not overshadow the player and will use mostly supportive behaviours to assist them \cite{DesignDocAIAllies}.

The combat actions for both the player and companion will be simple. This reduces scope and allows the AI to be incorporated in other action games easier. This is standard AI practice in industry \cite{GMTGoodAI, GDCLessIsMore, GDCSimplestAITrick}. While the player should be able to notice the AI, they should not need to rely on them for specific behaviours or completely change their play-style to get the AI to be helpful.

\section{Related Work}
\label{RelatedWork}

Research in 2010 highlights the lack of AI research in games \cite{RealTimeAICritique2010}. Since then, there has been a lot of improvements in AI research. Specifically looking at companion characters, there is a lot of research on implementing adaptive behaviour.

Friedman and Schrum analysed 2 companion bots in a first person shooter \cite{CompanionBotsFPS2019}. The ratings for the helpfulness of the companions was similar, which they said may have been due to vague terms. Adaptive agents that focus on being effective were rated as helpful due to the points they scored and their ability to survive better than non-adaptive AI. However, the non-adaptive agents that stayed near the player were also scored as helpful because they were seen in gameplay often. Players that rated the non-adaptive agents as more helpful justified this as they felt a stronger sense of teamwork and the game experience mattered more to them than points.

Geib et al. present a plan recognition approach that allows an agent to analyse the actions of another and use those actions to determine what plan is being attempted \cite{GeneratingCollabBehaviourPlanRecognition2016}. Unlike the agent by Friedman and Shrum \cite{CompanionBotsFPS2019}, this AI uses adaptive techniques to collaborate with another agent. This plan recognition agent adapts to the actions taken by another agent to determine their plan. Once a plan has been determined, the agent can work through the already completed steps to determine what still needs to be done and take actions towards these steps.

One of the more novel approaches, demonstrated by Tremblay and Verbugge, was to have a companion AI that can analyse the game intensity and change its behaviour as a way to dynamically adjust game difficulty \cite{tremblay2013adaptive}. Though this is not an example of AI adapting to another agent's actions, it still demonstrates the effect adaptability can have on the user experience.

In industry, games like \textit{God of War 2018}, \textit{The Last of Us} and \textit{Bioshock Infinite} are renowned for their AI companions \cite{PlayDontShow}. All three of the studios behind these games have developed AI techniques to fine-tune behaviour.

\subsection{Responsive Behaviour}
\label{Responsive Behaviour}

Atreus in \textit{God of War 2018} is a good example of synergistic AI, as he will help the player land slow attacks and will extend enemy vulnerability by shooting them with arrows \cite{GDCAtreus}. For example, when the player launches an enemy into the air, Atreus will shoot them and keep them there.

There is also a lot of emphasis on his character development over the course of the game, which is reflected in his behaviour. At the start of the game, he only takes actions when commanded, and learns to be more automated towards the middle of the game. Later, he becomes brash, so he starts to outright ignore the playersâ€™ commands, and performs actions that would otherwise only occur when commanded, like his special runic abilities.

These changes in his AI help to tie the narrative into gameplay and make him seem like a real character, which helps to allow the player to bond with Atreus. These changes are also noticeable when the player is separated from Atreus, which helps the players realise how much they rely on him.

% TOTO - Include other sources that talk about responsive behaviours

\subsection{AI Movement}
\label{Movement}

%Image of pathfinding with caption
\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{Images/IndustryResearch/TLOUPathfinding.png}
  
\caption{Pathfinding diagram in \textit{The Last of Us}}
\label{fig:TLOUPathfinding}
\end{figure}

Pathfinding is an important aspect of AI companions. The AI needs to be close to the player so that it is not forgotten \cite{GAIP2EllieAI}, but also not too close that it gets in their way and obstructs them \cite{CoupledEmpowermentMaximisation}. An example of poor AI pathfinding is in \textit{Skyrim} \cite{tremblay2013adaptive}, where the AI will move in front of the player when they are aiming and generally get in their way when moving. Naughty Dog addressed issues like this in \textit{The Last of Us} with pathfinding tools, shown in figure \ref{fig:TLOUPathfinding}, that keeps companions close to the player, but not in the way \cite{GAIP2EllieAI}. Ellie will also move out of the way if the player moves into her personal space. Vocal barks are used in such moments to add character, and it is a good practice to use voice acting to help make the player aware of the agentâ€™s actions, and make them feel more real \cite{GMTGoodAI}. However, they may not be suitable for games with lower budgets.

%Images of fight circles with captions
\begin{figure}
  \centering
  
  \begin{subfigure}[a]{\linewidth}
  \includegraphics[width=\linewidth]{Images/IndustryResearch/SpidermanKungFuCircle.png}
  \end{subfigure}
  
  \begin{subfigure}[b]{\linewidth}
  \includegraphics[width=\linewidth]{Images/IndustryResearch/KOARKungFuCircle.png}
  \end{subfigure}
  
  \caption{Demonstration of Kung-Fu Circles in (a) \textit{Marvel's Spiderman} and (b) \textit{Kingdoms of Amalur: Reckoning}}
  \label{fig:KungFuCircle}
\end{figure}

Many action games, such as \textit{Marvel's Spiderman} and \textit{Kingdoms of Amalur: Reckoning}, use a technique known as the Kung-Fu Circle to manage the positioning of multiple agents \cite{GAIPKungFuCircle, GDCSpiderman}, shown in figure \ref{fig:KungFuCircle}. An AI manager handles the positioning of enemies in this circle and manages when they can attack. While this technique is intended to manage multiple enemies in action games, it can be used to determine where the AI companion could be placed.

\subsection{Animations, Bespoke Behaviours and Call-outs}
\label{ABC}

A lot of industry practice with developing AI for companion characters is to use bespoke behaviour, detailed animations and vocal call-outs to give personality and character \cite{GAIP2EllieAI, GMTGoodAI, GAIPOReactions}. In particular, Irrational Games created a smart terrain system for \textit{Bioshock: Infinite} that allows \textit{Elizabeth} to interact with the environment \cite{GDCElizabeth, AIGamesBioshockAI}, shown in figure \ref{fig:BioshockSmartTerrain}. A lot of these finishing touches are a key part of making the companion feel more believable, allowing the player to empathise and engage with them more. It also helps to communicate NPC actions, so the player understands that they are actually making choices, otherwise they can miss the intelligence of the AI \cite{GMTGoodAI}.

Instead of focusing on animation and voice lines, the agent will use adaptive AI to improve the sense of collaboration between them and the player. The intent of this would be to improve player-companion relationships in games with lower budgets and to maximise them in games that can have animations and voice acting. To do this, it will feature AI techniques that allow a companion NPC to adapt to player actions to collaborate with them better.

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{Images/IndustryResearch/BioshockSmartTerrain.png}
  
\caption{Smart Terrain showcase in \textit{Bioshock: Infinite}}
\label{fig:BioshockSmartTerrain}
\end{figure}

\subsection{Collaboration Without Communication}
\label{Communication}

The core design, detailed in Section \ref{Background}, is that the player cannot explicitly command the companion. As a result, the AI needs to determine how to collaborate with the player without communication. The following methods are some ways in which an AI can collaborate with other agents without communication.

Board games like \textit{Hanabi} and \textit{Pandemic} are often used in academic research for collaborative AI as they involve teamwork between multiple players that cannot communicate with each-other. Eger and Grus demonstrate a technique for \textit{Hanabi} that uses timing to communicate between agents \cite{WaitASecond2019}. While this is effective for collaboration between AI agents, this method is not reliable for player-AI collaboration as a human player isn't going to perform actions at specific timings to communicate with an AI agent.

Walton-Rivers et al. present a an AI for \textit{Hanabi} that uses agent modelling to collaborate with other AI players \cite{EvaluatingHanabiAgents}. Agent modelling techniques involve observing actions taken by a player or another agent and using these actions to construct a model of them. This model describes how the observed entity acts and can be used by the AI determine its actions. Yannakakis et al. demonstrate the use of machine learning to determine how an agent can use a player model \cite{yannakakis2013playermodelling}.

Agent modelling can also be used in Real-Time Strategy (RTS) games. Schadd et al. demonstrate the use of agent modelling for \textit{Spring} \cite{OpponentModellingRTS2007}. Bakkes et al. then build upon this research to use it in Case-based game AI \cite{bakkes2009opponentmodelling}.

%\cite{van2005opponent} - This tends to not be used for commercial games however (though this source is from 2005 so it may be out of date)

Plan recognition is another common technique that is used in board games. This is another technique for agents that need to collaborate with other agents, including players, without requiring explicit commands or communication. Agents that use plan recognition observe the actions, similar to agent modelling. However, instead of constructing a model of the player, it uses these actions to their goals by comparing the steps required to achieve that goal to the steps that have been taken \cite{GeneratingCollabBehaviourPlanRecognition2016}. Once an entityâ€™s goal has been identified, the plan recognition agent will devise actions that can aid them in achieving their goals. The unfinished steps in the plan can be used as potential behaviours.

Sauma-ChacÃ³n and Eger evaluate the use of plan recognition agents in \textit{Pandemic} that can play with a human player \cite{PandemicPlanRecognition2021}. This AI was able to play at a level similar to that seen in full teams of AI and was perceived as more helpful. In addition, they detected a correlation between perceived helpfulness and skill.

Like agent modelling, this technique can also be used in RTS games as agents use observed behaviour to determine their allies' or opponents' plans. Jansen demonstrates the use of plan recognition as an ally in RTS games, instead of using it as an opponent \cite{PlayerAdaptiveRTSAI2007}. This agent can look at the player's actions and build units to support their plans.

\section{Proposed Research}
\label{ProposedResearch}

\subsection{Research Question \& Hypotheses}
\label{Hypotheses}

The use of adaptive AI companions can have an effect on various aspects of the game-play. The main aspect this research will test for is creating a sense of collaboration between the player and companion agent during combat and the question this research attempts to answer is \textit{Does the use of adaptive AI help to build player-companion collaboration?}

This question is the focus of the first hypothesis; the adaptive AI will be perceived as more collaborative than the non-adaptive AI. This will be tested in the experiment detailed in section \ref{ExperimentalDesign}. Comparing the rankings for the collaboration of both agent types will determine if there is a correlation between adaptability and collaboration.

The null hypothesis here is that adaptability has no discernible effect on the player's perception of collaboration. This will be determined if there is no significant correlation between the agent type and the rankings for collaboration.

The second hypothesis is that there will be a correlation between the rankings for the participantsâ€™ sense of collaboration and user experience. In an experiment by Friedman and Schrum, more detail in section \ref{RelatedWork}, some players rated the non-adaptive AI as more helpful was that it was seen more in gameplay, even if it scored lower in game \cite{CompanionBotsFPS2019}. Players justified this as it gave them a better player experience and sense of teamwork. 

Since the AI presented in this proposal focuses on collaborative behaviour and the experiment focuses on single combat encounters where the AI should be near the player at all times, it will be easier to test the effects of collaboration as both agents should be equally visible in combat. 

\subsection{Hypothesis Table}

\begin{tabular}{ |p{3cm}|p{3cm}|p{2cm}|  }
 \hline
 \multicolumn{3}{|c|}{Hypothesis Table} \\
 \hline
 Hypothesis & Null Hypothesis & Test\\
 \hline
    Adaptive companion will seen as more collaborative compared to the non-adaptive companion & 
    The adaptive agent is not rated as more collaborative than the non-adaptive agent &
    T-Test between the collaboration ratings of both agent types \\
 \hline
    Players' user experience will be improved if they felt a greater sense of collaboration & 
    There is a negative or no correlation between user experience and their sense of collaboration &
    T-Test between the user experience ratings of both agent types\\
 \hline
\end{tabular}

\section{Artefact}
\label{Artefact}

\subsection{Artefact Details}
\label{ArtefactDetails}

This research will present an adaptive AI for an action game responds the the player actions to collaborate with them. The intention is that this collaboration will improve the player experience, and this is one of the factors that will be tested, see section \ref{Hypotheses} for details on the hypotheses.

The agent will have general path-finding behaviours that keep it close to the player and it will try to stay within their vision so the player notices it. The AI will use a behaviour tree to determine its tasks. the tasks will mostly involve staying near the player and attacking nearby enemies.

This agent will feature opponent modelling, detailed in \ref{Communication}. When the player performs a combat action, the adaptive AI will be given information about what kind of action the player took and whether they were successful. It will use this information to construct a model that describes how the player acts. This model will be made up of key descriptors that describe the player's actions.

Every time the player takes an aggressive action, such as attacking, the AI will add to aggressive descriptor. However, if they miss, the AI will add to a panic descriptor. Each action will add to a specific descriptor and the amount they add will be tweaked in play-testing, see appendix \ref{AppendixQAPlan}. It will then use this player model in behaviour tree when deciding what it will do.

For example, if the player is aggressive, but are getting hit a lot, the companion AI may focus on distracting other enemies from the player so they can continue to focus on their own play-style. However, if the player is very defensive, and tends to attack only after an enemy attacks, the adaptive AI may focus on the player's current target and create an opening for them to attack. If the player is getting overwhelmed by enemies, the agent may try to draw enemies away from the player to give them some space to relax.

A plan recognition approach would have required the player to think about their strategy more, which would suit a slower or more strategic game. An action game is faster paced, and because the player does not have as much time to think, they won't be able to come up with plans. As a result, this project will feature a player modelling approach because it can work better with a simpler combat system, but still enables adaptive behaviour.

\subsection{System Development Life-cycle}
\label{DevLifecycle}

The project uses version control to track changes, and feature branches are used to organize features and resolve conflicts. In addition, a task board has been made, and will be used to track the progress of the development.

A basic combat system has been created with AI agents that chase the closest enemy and attack when they are in range. The player has basic combat actions that are standard in most action games; attacks, parries and dodges. There is also a basic prototype of a player modelling AI, which can take actions performed by the player and give a basic descriptor that defines their dominant play-style.

The next feature planned is setting up a behaviour tree and tasks that will make designing AI behaviours quicker. Once a behaviour tree is implemented, the simple AI for the enemies and companion will be developed, and then the adaptive companion AI will be worked on.

The adaptive companion will be the focus of the research, and most of the development time will be spend on this. The player model construction will need to be refined from its prototype and then it will need to be able to use this player model in its decision-making algorithm.

\subsection{Validation and Verification}
\label{Validation}

These behaviours will be tested with pilot play-testing to ensure that the player model is constructed properly and that appropriate actions are taken. This play-testing will also give me opinions on whether the aforementioned AI behaviours are beneficial for the player experience and will allow me to adjust behaviours before the experiment. In addition, unit tests will be used to confirm the player model is properly constructed and the correct behaviours are taken based on this model. Refer to appendix \ref{AppendixQAPlan} for more details on the quality assurance test plan.

\section{Research Method}
\label{ResearchMethod}

\subsection{Philosophical Position}
\label{PhilosophicalPosition}

%What is this thing called Science? https://falmouth.primo.exlibrisgroup.com/discovery/fulldisplay?context=L&vid=44FAL_INST:44FAL_VU1&search_scope=MyInst_and_CI&tab=Everything&docid=alma9911072374905136
%Scientific Method https://plato.stanford.edu/entries/scientific-method/
%Philosophy of Science https://undsci.berkeley.edu/the-philosophy-of-science/
%Philosophy and Paradigm of Scientific Research https://www.intechopen.com/chapters/58890 

%TODO Mid - research empirical philosophy to see if it fits better
%TODO Justify philosophies

The research will be carried out using positivist philosophies \cite{Zukauskas18}. A survey will be used to collect quantitative data that analyses the effect of AI adaptability on the player's collaboration the AI agent, and how that affects their opinions on the AI. There may be some questions with qualitative answers, but these will be used to assure that there are no issues with the study.

\subsection{Experimental Design}
\label{ExperimentalDesign}

%Might be best to have each participant play both as they could be skewed by personal biases so the baseline needs to be established randomise the order

%G-Power video explanation: https://web.microsoftstream.com/video/d1ec1c56-bb97-4592-ae4e-1c973e3fee20?referrer=https:%2F%2Flearningspace.falmouth.ac.uk%2F

The experiment will take place on campus, either at the Games Academy Warehouse or the Design Labs and members of staff will be contacted to get permission to use these spaces for the experiment. If there are not enough responses, see section \ref{DataManagement} for sample size, the project and forms will be sent online to get more responses.

Two companion agents will be set up in an action game. The first agent will feature a simple AI which will simply attack the closest target with random attacks and use basic path-finding to keep close to the player when not attacking. The second agent will feature an adaptive AI, which will determine targets based on the player actions and will choose attacks that support the player's intentions better.

Structured observations and questionnaires will be used to collect data on the AI agents. Each participant will fill out an initial survey detailing their experience with games and any general opinions on companions in games.

After completing this survey, they will play through a demo that features two combat encounters. In one encounter, the participant will be assisted by the simple AI agent, while the other encounter will have them assisted by an adaptive AI agent. To avoid observer bias, the order the agents feature will be randomly selected \cite{hrobjartsson2013observer}.

Once they have played through the demo, the participants will be given a questionnaire form to fill out. Some of the questions would start out establishing the participant's prior experience with games and what kind of games they like, and will then move onto the specifics in the AI.

There is a link to a draft of the questionnaire in appendix \ref{AppendixLinks}.

\subsection{Data Management Plan}
\label{DataManagement}

Most of the questions will use a Likert scale to distil responses into quantitative data and will include questions on how likeable, intelligent, collaborative, etc. the player thought they were, using a similar approach used by Z. Ashktorab et al. \cite{SocialPerceptions2020}. There will be some qualitative questions that allow participants to put sentence answers so they can give more specific thoughts. This will also help to determine if there are any bugs that caused one AI to not work as intended.

The questions will use a 6-point Likert scale, this means that they will not have a neutral value. This will result in the answer always being useful. Additionally, using a 6-point scale will give more accuracy as it reduces a central tendency bias (cite likert scale). This is caused when participants avoid choosing extreme responses to avoid seeming like they have extremist values. Having a point scale more than 7 may be less manageable for participants

The responses to the questionnaire will be converted to a csv file, and stored on an online network drive. The draft form is currently a Google form, but this will be changed to a Microsoft form because the free version for Google forms is not GDRP compliant, which presents a data risk \ref{AppendixLinks}. This data will then be analysed using R Studio. This will output various graphs that will visualise the data for readers and I will also perform statistical tests to prove or disprove the hypotheses, detailed in section \ref{Hypotheses}. Both the R-Code and graphs with sample data are in appendix \ref{AppendixRCode}.

Both hypotheses will be tested using T-Tests. Using G-Power, a sample size of 59 is required for T-Tests with an effect size of 0.4. This effect size was chosen because it is a medium-large effect size, as defined by Cohen \cite{cohen1988statistical}. A small or medium effect size may not be noticed by most players, especially in a fast-paced action game, as they may be more focused on their own behaviours.

%G-Power downloads
    %https://www.psychologie.hhu.de/arbeitsgruppen/allgemeine-psychologie-und-arbeitspsychologie/gpower
    %https://learningspace.falmouth.ac.uk/mod/resource/view.php?id=245797

\subsection{Ethical Considerations}
\label{EthicalConsiderations}

The data collection will involve an experiment that focuses on various AI behaviours, and how different behaviours can build a stronger emotional connection between players and AI companion NPCs. As such, participants will need to be involved. Participants will test the AI and fill out survey forms to give their opinions on the various AI mechanics. Under these criteria, there are no high risk categories, but since participants are involved, it can count as a medium risk experiment. To protect the rights of the participants, the Falmouth University guidelines and the BCS Code of Conduct have been followed \cite{BCSConductCode}.

The participants will be given information forms that detail the experiment and their participation in it. There are no aspects of the experiment that are going to be hidden from the participants, and the form will have clear information on what the experiment will entail. The form will have information on the purpose of the research, expected duration, procedures, what the participants are being asked to do, their right to decline and withdraw, confidentiality and contact info for the researcher.

In order to participate within the experiment, the participant will be asked to sign that they understand and agree with the form, no signature will be required. Participants will also be required to state that they are at least 18 years of age to participate with the experiment. However they will not need to state their age, only that they are above 18 years old. There will also be no transactions or other coercion to participate with the experiment.

All participants will be given a right to withdraw at any time and this will also be made clear in the information and consent forms. The forms will have contact information for them and a reference number so that their responses can be removed without requiring them to give any personal information. They will also be able to use this number to check their responses, though they will not be able to change their results once submitted to preserve pure data.

The questions on the survey forms will not have any questions that require the user to input personal information. The questions are mostly focused on their opinions of the AI, though they may be asked about their opinions on AI in games and how often they play games.

Most of the questions will be using a Likert scale, and there will be few qualitative questions that allow them to express more opinions. None of the questions will require the participants to divulge any personal information. Once the data has been analysed, it will be archived until the study is complete, after which all responses will be deleted.

\section{Expected Outcomes}
\label{ExpectedOutcomes}

It is expected that incorporating an agent modelling AI would increase the player's sense of collaboration with a companion character and that this collaboration will have a positive effect on the user experience. However, this will be determined by the design of the AI behaviours, as they need to be distinct enough to make the adaptive behaviours worthwhile.

\section*{Acknowledgments}

I would like to thank the staff at Falmouth University for their support. In particular, I'd like to thank my project supervisor, Joseph Walton-Rivers, for his guidance on the project, as well as Michael Scott, the module leader for the dissertation modules.

%I would also like to thank everyone that participated in the playtesting.

\bibliographystyle{IEEEtran}
\bibliography{bibliography} 

 \newpage
 
\section{Appendices}
\label{Appendices}

\subsection{Appendix - Links}
\label{AppendixLinks}

This is the link to the Github Repository: \url{https://github.falmouth.ac.uk/Games-Academy-Student-Work-22-23/Buddy-NPC-Dissertation}

The draft for the questionnaire was created in Google forms, but this will not be used in the experiment as it could present a data risk, more details in section \ref{DataManagement}. This is the link to the questionnaire draft, : \url{https://docs.google.com/forms/d/e/1FAIpQLSeOhC7PEpuYuBs3D_8r5LFBvWAq_hwGGgM-3Jaw7EKlvTLH9A/viewform?usp=sf_link}


\subsection{Appendix - Quality Assurance Plan}
\label{AppendixQAPlan}
%(Perry 1987)
%Correctness, Reliability, Efficiency, Integrity, Usability, Maintainability, Testability, Flexibility, Portability, Interoperability

%Types of Testing: Unit Testing, Automation and Continuous Integration, Run-Time Analyses, End-User Testing

The artefact will be tested in multiple stages. First, the basic combat mechanics and enemy AI will be playtested with end-users, which would most likely be friends and family, to ensure that there are no bugs that could interfere with the results of the experiment. I will gather vocal feedback and track any issues on a Trello board, which will be used to mark progress.

This is an important aspect as the basic mechanics will determine how the player interacts with the game and the decision making for the companion agent relies on this.

When the basic combat mechanics and enemy AI are tested, the adaptive AI will go into testing. Unit tests will be used to confirm the calculations for generating the player model are correct and that the correct behaviours are chosen for the adaptive AI.

Once the adaptive agent has passed the unit testing, run-time testing will be used for optimisation. The aim is for the game to run at a minimum of 30 fps on the machines in the Games Academy. If the game isn't performant enough, the profiler in Unity will be used to find bottlenecks and fix them.

Before conducting the experiment, the game will be tested again with friends and family to ensure that the adaptive AI is behaving properly and to regulate difficulty. While game balance is not a primary concern here, the game should be challenging enough so that the companion provides clear use for the player, but not overwhelming to distract the player from noticing them.

Throughout this process, regular builds will be made and tested to ensure that it works. Automated build generation may be used to make this process quicker.

Additionally, generating reference codes will need to be tested to ensure that multiple participants will not be given the same code. The initial plan for constructing the reference code is to construct multiple sets of digits. The first set of digits could be the current time, then next set could be randomly generated and the final digits could be determined by which AI the participant was given first. This system will need to be tested properly when it is developed.

\subsection{Appendix - Graphs with Random Data}

Figures \ref{fig:LikabilityRplot} and \ref{fig:IntelligenceLikabilityRplot} have been created using the R-Code in appendix \ref{AppendixRCode}. Random data has been put into these graphs to show a sample of what the graphs would look like.

Figure \ref{fig:LikabilityRplot} displays a box plot that compares the likability between the 2 agent types while figure \ref{fig:IntelligenceLikabilityRplot} shows a q plot that displays how the perceived intelligence of the AI relates to the ranking for likability.

\label{AppendixGraphs}
\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{Images/Graphs/LikabilityRplot.png}
  
\caption{Comparing the likability between companion types (sample data)}
\label{fig:LikabilityRplot}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{Images/Graphs/IntelligenceLikabilityRplot.png}
  
\caption{Comparing the effect perceived intelligence has on likability (sample data)}
\label{fig:IntelligenceLikabilityRplot}
\end{figure}

\onecolumn
\subsection{Appendix - R-Code}
\label{AppendixRCode}

\begin{verbatim}
    #Setting up the libraries
library(tidyverse)
library(psych)
library(readr)
library(ggplot2)
library(MASS)
library(broom)

#Getting the data, view gets data file but cannot be compiled
rawData <- read_csv("obfuscated_data.csv")
glimpse(rawData)

#Replace the labels
data <- rawData %>%
  mutate(AgentType = factor(AgentType, levels = c("1", "2"),
                            labels = c("Adaptive AI", "Non-Adaptive AI"))) %>%
glimpse(data)

#Creates a data frame for mean values and standard deviation
sumData <- data %>%
  group_by(AgentType) %>%
  summarize(likeabilityMean = mean(LIKEABILITY),
            likeabilityError = sd(LIKEABILITY)/sqrt(n()),
            intelligenceMean = mean(INTELLIGENCE),
            intelligenceError = sd(INTELLIGENCE)/sqrt(n()),
            adaptabilityMean = mean(ADAPTABILITY),
            adaptabilityError = sd(ADAPTABILITY)/sqrt(n()),
            sampleCount = n())%>%
  ungroup() %>%
sumData

#Group Data by Agent Type
adaptiveData <- subset(rawData, AgentType == "1") %>%
  mutate(AgentType = factor(AgentType, levels = c("1", "2"), 
                            labels = c("Adaptive AI", "Non-Adaptive AI"))) %>%
adaptiveData

nonData <- subset(rawData, AgentType == "2") %>%
  mutate(AgentType = factor(AgentType, levels = c("1", "2"), 
                            labels = c("Adaptive AI", "Non-Adaptive AI"))) %>%
nonData

#T Tests
#T Test for Hypothesis 1 - Adaptive companion will seen as more collaborative 
                          #compared to the non-adaptive companion
likeTest <- t.test(adaptiveData$COLLABORATIVE, nonData$COLLABORATIVE, 
                   alternative="greater") %>%
likeTest

#T Test for Hypothesis 2 - Players' user experience will be improved if they 
                            #felt a greater sense of collaboration
likeTest <- t.test(adaptiveData$USEREXPERIENCE, nonData$USEREXPERIENCE, 
                   alternative="greater") %>%
likeTest

#Summarize the values
#Plots for for adaptability, likability and intelligence ratings for the agent types

#Adaptability
data %>%
  group_by(AgentType) %>%
  summarise(min = min(ADAPTABILITY), median = median(ADAPTABILITY), max = max(ADAPTABILITY))
#Box Plot
qplot(factor(data$AgentType), data$ADAPTABILITY, geom = "boxplot", 
      main="Comparing the adaptability rating of the companions", 
      xlab="AI Type", ylab="Adaptability")
#Plot for mean
ggplot(data = sumData,
       aes(x = AgentType, y = adaptabilityMean, group = AgentType, color = AgentType)) +
  geom_point(size = 5)

#Likeability
data %>%
  group_by(AgentType) %>%
  summarise(min = min(LIKEABILITY), median = median(LIKEABILITY), max = max(LIKEABILITY))
#Box Plot
qplot(factor(data$AgentType), data$LIKEABILITY, geom = "boxplot", 
      main="Comparing the likability rating of the companions", 
      xlab="AI Type", ylab="Likability")
#Plot for mean
ggplot(data = sumData,
       aes(x = AgentType, y = likeabilityMean, group = AgentType, color = AgentType)) +
  geom_point(size = 5)

#Intelligence
data %>%
  group_by(AgentType) %>%
  summarise(min = min(INTELLIGENCE), median = median(INTELLIGENCE), max = max(INTELLIGENCE))
#Box Plot
qplot(factor(data$AgentType), data$INTELLIGENCE, geom = "boxplot", 
      main="Comparing the intelligence rating of the companions", 
      xlab="AI Type", ylab="Intelligence")
#Plot for mean
ggplot(data = sumData,
       aes(x = AgentType, y = intelligenceMean, group = AgentType, color = AgentType)) +
  geom_point(size = 5)

#Mapping Adaptability to Intelligence
qplot(data$ADAPTABILITY, data$INTELLIGENCE, geom = c("point", "smooth"), method = "rlm",
      main = "Correlation between ratings for Adaptability and Intelligence", 
      xlab = "Adaptability", ylab = "Intelligence")

#Mapping Intelligence to Likeability
qplot(data$INTELLIGENCE, data$LIKEABILITY, geom = c("point", "smooth"), method = "rlm",
      main = "Correlation between ratings for Intelligence and Likeability", 
      xlab = "Intelligence", ylab = "Likeability")

#Mapping Adaptability to Likeability
qplot(data$ADAPTABILITY, data$LIKEABILITY, geom = c("point", "smooth"), method = "rlm",
      main = "Correlation between ratings for Adaptability and Likeability", 
      xlab = "Adaptability", ylab = "Likeability")


\end{verbatim}

\end{document}